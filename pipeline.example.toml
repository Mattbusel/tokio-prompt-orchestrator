# Example pipeline configuration for tokio-prompt-orchestrator
#
# Usage: cargo run -- --config pipeline.example.toml
#
# Generate JSON Schema for IDE autocomplete:
#   cargo run -- schema > pipeline.schema.json

[pipeline]
name = "production"
version = "1.0"
description = "Production pipeline with OpenAI GPT-4"

# ── Stage Configuration ──────────────────────────────────────────────────

[stages.rag]
enabled = true
timeout_ms = 5000
max_context_tokens = 2048
# channel_capacity = 512  # optional, defaults to 512

[stages.assemble]
enabled = true
# channel_capacity = 512  # defaults to 512

[stages.inference]
worker = "open_ai"
model = "gpt-4"
max_tokens = 1024
temperature = 0.7
timeout_ms = 30000

[stages.post_process]
enabled = true

[stages.stream]
enabled = true

# ── Resilience ───────────────────────────────────────────────────────────

[resilience]
retry_attempts = 3
retry_base_ms = 100          # exponential backoff base
retry_max_ms = 5000          # backoff cap
circuit_breaker_threshold = 5
circuit_breaker_timeout_s = 60
circuit_breaker_success_rate = 0.8

# ── Deduplication ────────────────────────────────────────────────────────

[deduplication]
enabled = true
window_s = 300               # 5-minute dedup window
max_entries = 10000

# ── Observability ────────────────────────────────────────────────────────

[observability]
log_format = "json"
metrics_port = 9090
# tracing_endpoint = "http://jaeger:14268"  # optional OTLP endpoint
